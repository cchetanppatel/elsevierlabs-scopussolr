input {
  file {
    # A type is a label applied to an event. It is used later with filters
    # to restrict what filters are run against each event.
    format => "plain"
    path => "<KIBANA_LOGFILE>"
    type => "<KIBANA_LOGTYPE>"
    start_position => "end"
    sincedb_path => "/hothouse/.sincedb"
    debug => "true"
  }
}

# Sample records that we want to process (drop any other lines in the file
# QUERY_STATS loadQuery 500 TS Tue, 03 Sep 2013 13:07:27 GMT RES_TIME 153 SOLR_TIME - LEN - SOLR_LEN - DTYPE auth DSET user DIDX 5 HITS - POST
# QUERY_STATS loadQuery 200 TS Wed, 11 Sep 2013 19:00:20 GMT RES_TIME 1619 SOLR_TIME 7 LEN 2303 SOLR_LEN 666 DTYPE affil DSET afid DIDX 1 HITS 1 POST
#

filter {
  grep {
    match => [ "@message", "QUERY_STATS loadQuery*" ]
  }
  # Flag the records that will have all the timing info so we can generate averages, etc.
  grep {
    match => [ "@message", "QUERY_STATS loadQuery 200*" ]
    drop => false
    add_tag => ["200-rc"]
  }
  # Flag the records that will not have all the timing info so we don't try and extract them, etc.
  grep {
    match => [ "@message", "QUERY_STATS loadQuery 200*" ]
    drop => false
    negate => true
    add_tag => ["non-200-rc"]
  }
  # Extract out the timestamp to associate with this record
  grok {
    pattern => "QUERY_STATS loadQuery %{BASE10NUM} TS %{DAY}, %{BASE10NUM:date} %{MONTH:month} %{YEAR:year} %{HAPROXYTIME:time} GMT*"
    add_field => ["qts", "%{month} %{date} %{year} %{time} Z" ]
  }
  # Reset the record's timestamp (that would be when the record was read from the file) to match the extracted timestamp
  date {
    match => [ "qts", "MMM dd YYYY HH:mm:ss Z"]
  }
  # Process 200 rc status records
  # QUERY_STATS loadQuery 200 TS Wed, 11 Sep 2013 19:00:20 GMT RES_TIME 1619 SOLR_TIME 7 LEN 2303 SOLR_LEN 666 DTYPE auth DSET user DIDX 1 HITS 1 POST
  grok {
    pattern => "QUERY_STATS loadQuery %{BASE10NUM:status:int} TS %{DAY}, %{BASE10NUM:date} %{MONTH:month} %{YEAR:year} %{HAPROXYTIME:time} GMT RES_TIME %{BASE10NUM:restime:int} SOLR_TIME %{BASE10NUM:estime:int} LEN %{BASE10NUM:bytes:int} SOLR_LEN %{BASE10NUM:esbytes:int} DTYPE %{WORD:dType} DSET %{WORD:dSet} DIDX %{BASE10NUM:dIdx:int} HITS %{BASE10NUM:hits:int} %{WORD:method}"
    #add_field => ["qts", "%{month} %{date} %{year} %{time} Z" ]
    tags => ["200-rc"]
  }


  # Process non-200 rc status records
  grok {
    pattern => "QUERY_STATS loadQuery %{BASE10NUM:status:int} TS %{DAY}, %{BASE10NUM:date} %{MONTH:month} %{YEAR:year} %{HAPROXYTIME:time} GMT RES_TIME %{BASE10NUM} SOLR_TIME %{BASE10NUM} LEN %{NOTSPACE} SOLR_LEN %{NOTSPACE} DTYPE %{WORD:dType} DSET %{WORD:dSet} DIDX %{BASE10NUM:dIdx:int} HITS %{NOTSPACE} %{WORD:method}"
    tags => ["non-200-rc"]
  }
  # Get rid of all the extra tags that were parsed out that we don't care about in Kibana
  mutate {
    remove => [ "date", "month", "year", "time", "haproxy_hour", "haproxy_minute", "haproxy_second"  ]
  }
}

output {
  # Print each event to stdout.
  #stdout {
    # Enabling 'debug' on the stdout output will make logstash pretty-print the
    # entire event as something similar to a JSON representation.
    # debug => true
  #}
  elasticsearch_http {
    host => "<KIBANA_HOST>"
    index => "solr-query-logstash-%{+YYYY.MM.dd}"
    flush_size => 1
    exclude_tags => [ "_grokparsefailure" ]
  }
}
